---
title: "ZAD 4 - tidyverse - dostrojenie innego hiper-parametru drzewa decyzyjnego"
author: "Jakub Ciura"
format: 
  html:
    theme: minty            
    toc: true                 
    toc-depth: 2              
    code-fold: true           
    code-tools: true          
    number-sections: true     
    font:                     
      text: "Lato"            
      code: "Fira Code"       
      heading: "Roboto"       
    page-layout: full        
editor_options: 
  chunk_output_type: console
abstract: |
  Są to zautomatyzowane pomiary analizy obrazu, które są najważniejsze w przewidywaniu jakości segmentacji. 
  Pozostawiam czytelnikowi zbadanie, czy można dostroić inny hiper-parametr drzewa decyzyjnego. Możesz zapoznać się z dokumentacją referencyjną lub skorzystać z args() funkcji, aby sprawdzić, które argumenty obiektu pasternak są dostępne:
execute:
  echo: true                 
  warning: false             
  message: false
cache: true          
---

# Przygotowanie danych
Analogicznie jak w przypadku dokumentu "Zad3 - tidyverse", zadanie rozpoczynamy od załadowania danych oraz wstępnej ich konwersji. Poszczególne procedury opisane są w dokumencie "Zad2 - tidyverse".

```{r}
#| results: "hide"
#| warning: false
#| message: false

library(tidymodels) 
library(skimr) 
library(GGally) 
library(openair) 
tidymodels_prefer()

air <- mydata |> selectByDate(year = 2004) 
air |> skim()

air <- air |> na.omit()

library(ggpubr)

air |> 
  pull(o3) |> 
  range()  

air <-
  air |>
  mutate(ozone = cut(
    o3,
    breaks = c(-0.1, 10, 53),
    labels = c("Niskie", "Wysokie")
  ))

air |> count(ozone)

split <- initial_split(air, strata = ozone, prop = 0.7)  # 70% na trening, 30% na test
train_data <- training(split)
test_data <- testing(split)

```

```{r}
library(randomForest)

# Definiowanie modelu regresji logistycznej
log_model <- logistic_reg() |> 
  set_engine("glm") |> 
  set_mode("classification")

# Definiowanie modelu lasu losowego
rf_model <- rand_forest() |> 
  set_engine("randomForest") |> 
  set_mode("classification")

```

```{r}
# Przygotowanie przepisu dla modelu regresji logistycznej
log_recipe <- recipe(ozone ~ nox + no2, data = train_data) |> 
  step_normalize(all_predictors())  # Normalizacja zmiennych niezależnych

# Przygotowanie przepisu dla modelu lasu losowego
rf_recipe <- recipe(ozone ~ nox + no2, data = train_data) |> 
  step_normalize(all_predictors())  # Normalizacja zmiennych niezależnych

```


# Badanie argumentów drzewa decyzyjnego

Dostępne są argumenty:
```{r}
args(decision_tree)
```

Jako hiperparametr w tym zadaniu wybrane zostało *tree_depth* - parametr, definiujący maksymalną głębokość drzewa, wpływający na dokładność oraz możliwość przeuczenia modelu.

# Konfiguracja modelu z tuningiem oraz zmianą hiperparametru
```{r}

# CV, 5 krotna
cv_folds <- vfold_cv(train_data, v = 5, strata = ozone)  

# Konfiguracja modelu z tuningiem
tree_spec <- decision_tree(
  tree_depth = tune()  # Wybór hiperparametru do strojenia
) %>%
  set_engine("rpart") %>%
  set_mode("classification")

# Definicja przepływu pracy z wybraną receptą
tree_workflow <- workflow() %>%
  add_model(tree_spec) %>%
  add_recipe(log_recipe)

# Definicja siatki wartości dla `tree_depth`
tree_grid <- grid_regular(tree_depth(), levels = 10)

# Strojenie modelu
tune_results <- tune_grid(
  tree_workflow,
  resamples = cv_folds,
  grid = tree_grid,
  metrics = metric_set(roc_auc, accuracy)
)

# Wyniki
collect_metrics(tune_results)

```

# Wnioski

Jak można zauważyć, zmiany *tree_depth* powodują zmianę dokładności oraz ROC AUC. Wartości 2 i 3 powodują pogorszenie działania modelu, jednak zwiększenie do wartości 4 powoduje polepszenie jego działania. Dalsze zwiększanie tego parametru nie powoduje poprawy.




